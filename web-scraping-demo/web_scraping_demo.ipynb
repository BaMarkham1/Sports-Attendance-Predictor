{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries, set the url\n",
    "#pandas is used to easily grab tables from the page\n",
    "import pandas as pd\n",
    "#selenium is used to get pages that require javascript code to be executed to get data\n",
    "from selenium import webdriver\n",
    "#this is used for parsing information from the html code\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "#this is where the chrome driver is located\n",
    "driver_path = r\"C:/Users/lemar/Desktop/schoolwork/Senior Design 2/chromedriver_win32/chromedriver.exe\"\n",
    "#set the path to the chrome driver\n",
    "browser = webdriver.Chrome(executable_path = driver_path)\n",
    "#this is the url we'll visit\n",
    "url = \"https://www.pro-football-reference.com/teams/den/2019_advanced.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, let's get the tables directly using pandas\n",
    "#this method returns a list of tables that were on the page\n",
    "tables = pd.read_html(url)\n",
    "#print out the tables to see\n",
    "print(\"Total of \" + str(len(tables)) + \" tables\")\n",
    "print()\n",
    "for table_index, table in enumerate(tables):\n",
    "    print(\"Table \" + str(table_index))\n",
    "    print(table.head())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that the above only shows 4 tables. It only grabbed the first 4 on the page concerning QB's.\n",
    "#that's because the read_html method (and most methods for requesting web pages) will only return\n",
    "#static html content, not any html that is created by javascript code.\n",
    "#to fix that, we use selenium\n",
    "#first, get the page using Selenum:\n",
    "browser.get(url)\n",
    "#now pass that to the read_html function\n",
    "post_js_tables = pd.read_html(browser.page_source)\n",
    "#now print the tables again to see the difference\n",
    "print(\"Total of \" + str(len(post_js_tables)) + \" tables\")\n",
    "print()\n",
    "for table_index, table in enumerate(post_js_tables):\n",
    "    print(\"Table \" + str(table_index))\n",
    "    print(table.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's say we need to get the links in one of the tables\n",
    "#this is where we'll utilize BeautifulSoup\n",
    "#we can inspect the html in Chrome using More tools -> Developer tools to find the id of the table we want\n",
    "#here's the id we're looking for in this case:\n",
    "table_id = \"advanced_rushing\"\n",
    "#get just that table\n",
    "table_tag =  browser.find_element_by_id(table_id)\n",
    "#get the html within the table\n",
    "inner_html = table_tag.get_attribute(\"innerHTML\")\n",
    "#show the raw html\n",
    "print(inner_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all of the links in the table's inner html as a list\n",
    "a_tags =  BeautifulSoup(inner_html, parse_only=SoupStrainer('a'), features=\"lxml\")\n",
    "#print the links\n",
    "for each in a_tags:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's parse the tag to get the url within it\n",
    "#this list will hold the links contained in the a tags\n",
    "links = []\n",
    "#for each a tag,\n",
    "for each in a_tags:\n",
    "    #get what comes after the equals sign\n",
    "    first_split = str(each).split(\"=\")[1]\n",
    "    #get what comes before the closing symbol for the a tag\n",
    "    second_split = first_split.split(\">\")[0]\n",
    "    #take off the quotes by cutting out the first and last characters\n",
    "    links.append(second_split[1:-1])\n",
    "#print the parsed links\n",
    "for each in links:\n",
    "    print(each)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
